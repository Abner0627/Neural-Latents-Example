{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Import Packages"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","import torch\n","import torch.optim as optim\n","import torch.nn as nn\n","import time\n","import os\n","from scipy.ndimage import gaussian_filter1d\n","from nlb_tools.make_tensors import h5_to_dict, save_to_h5\n","from nlb_tools.evaluation import evaluate\n","import h5py"]},{"cell_type":"markdown","metadata":{},"source":["## GPU Setting"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]}],"source":["tStart = time.time()\n","print(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') "]},{"cell_type":"markdown","metadata":{},"source":["## Sub Functions"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["def _GF(ddd, sig, N, C, log=True):\n","    TL = []\n","    for n in range(N):\n","        CL = []\n","        spike = ddd[n, :, :]\n","        for c in range(C):\n","            sp_c = spike[:, c]\n","            sp_c_gf = gaussian_filter1d(sp_c.astype(np.float32), sig)\n","            if log:\n","                CL.append(np.log(sp_c_gf + 1e-10)[:, np.newaxis])\n","            else:\n","                CL.append(sp_c_gf[:, np.newaxis])\n","        TL.append(np.hstack(CL)[np.newaxis, :, :])\n","    TL = np.vstack(TL)\n","    return TL\n","\n","def _Dict(dataset_name, train, valid, T, C):\n","    train_rates_heldin = train[:, :T, :C]\n","    train_rates_heldout = train[:, :T, C:]\n","    eval_rates_heldin = valid[:, :T, :C]\n","    eval_rates_heldout = valid[:, :T, C:]\n","    eval_rates_heldin_forward = valid[:, T:, :C]\n","    eval_rates_heldout_forward = valid[:, T:, C:]\n","\n","    output_dict = {\n","        dataset_name: {\n","            'train_rates_heldin': train_rates_heldin.astype(np.float64),\n","            'train_rates_heldout': train_rates_heldout.astype(np.float64),\n","            'eval_rates_heldin': eval_rates_heldin.astype(np.float64),\n","            'eval_rates_heldout': eval_rates_heldout.astype(np.float64),\n","            'eval_rates_heldin_forward': eval_rates_heldin_forward.astype(np.float64),\n","            'eval_rates_heldout_forward': eval_rates_heldout_forward.astype(np.float64)\n","        }\n","    }  \n","    return output_dict\n","\n","def _shuffle(spk_hi):\n","    spk_hi_shf = spk_hi[:, :, torch.randperm(spk_hi.size()[2])]\n","    return spk_hi_shf\n","\n","def _GF_pt(spk_pt, sig, N, C, log=True):\n","    spk_np = spk_pt.data.numpy()\n","    spk_GF_np = _GF(spk_np, sig, N, C, log)\n","    return torch.from_numpy(spk_GF_np).type(torch.FloatTensor)\n","\n","def init_weights(m):\n","    if isinstance(m, nn.Linear):\n","        torch.nn.init.xavier_uniform_(m.weight)\n","        m.bias.data.fill_(0.01)"]},{"cell_type":"markdown","metadata":{},"source":["## Create Dictionary"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","==========mc_maze==========\n"]}],"source":["dataset_dict = {\n","    '000128': 'mc_maze',\n","    '000127': 'area2_bump',\n","    '000130': 'dmfc_rsg',\n","    '000129': 'mc_rtt',\n","    '000138': 'mc_maze_large'\n","}\n","hT_dict = {\n","    '000128': 7,\n","    '000127': 6,\n","    '000130': 15,\n","    '000129': 6,\n","    '000138': 7\n","}\n","sP = './inFR'\n","dpath = './data/h5'\n","data_list = os.listdir('./data/h5')\n","\n","# for idx in ['000127', '000128', '000129', '000130', '000138']:\n","idx = '000128'\n","dataset_name = dataset_dict[idx]\n","print('\\n==========' + dataset_name + '==========')\n","for d in data_list:\n","    if d.split('_')[0]==idx:\n","        if d.split('_')[1]=='train':\n","            train_F = d\n","        elif d.split('_')[1]=='eval':\n","            valid_F = d\n","        elif d.split('_')[1]=='test':\n","            test_F = d   \n","        elif d.split('_')[1]=='target':\n","            target_F = d               "]},{"cell_type":"markdown","metadata":{},"source":["## Data Processing"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["train_H5 = h5py.File(os.path.join(dpath, train_F), 'r')\n","valid_H5 = h5py.File(os.path.join(dpath, valid_F), 'r')\n","test_H5 = h5py.File(os.path.join(dpath, test_F), 'r')\n","target_dict = h5_to_dict(h5py.File(os.path.join(dpath, target_F), 'r'))\n","\n","train_spikes_heldin  = np.array(train_H5['train_spikes_heldin'])\n","train_spikes_heldin_forward  = np.array(train_H5['train_spikes_heldin_forward'])\n","train_spikes_heldout  = np.array(train_H5['train_spikes_heldout'])\n","train_spikes_heldout_forward  = np.array(train_H5['train_spikes_heldout_forward'])\n","\n","eval_spikes_heldin = np.array(valid_H5['eval_spikes_heldin'])\n","eval_spikes_heldout = np.array(valid_H5['eval_spikes_heldout'])\n","\n","test_spikes_heldin = np.array(test_H5['eval_spikes_heldin'])\n","\n","N_tra, T, C = train_spikes_heldin.shape\n","N_val = eval_spikes_heldin.shape[0]\n","N_tes = test_spikes_heldin.shape[0]\n","_, To, Co = train_spikes_heldout_forward.shape"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["train_fd = np.concatenate([train_spikes_heldin_forward, train_spikes_heldout_forward], axis=-1).astype(np.int64)\n","train_bd = np.concatenate([train_spikes_heldin, train_spikes_heldout], axis=-1).astype(np.int64)\n","train = np.concatenate([train_fd, train_bd], axis=1)"]},{"cell_type":"markdown","metadata":{},"source":["## Pack Data"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["from model_seq2seq import GRU_AT_AE, GRU_HoFw \n","C1 = C//2\n","C2 = C - C1\n","out_sz = 128  \n","model_AE = GRU_AT_AE(C//2, C2, out_sz)\n","model_HOFW = GRU_HoFw(T, C, To, Co, hT_dict[idx])\n","\n","model_AE.apply(init_weights)\n","model_HOFW.apply(init_weights)\n","\n","LOSS_TRA = []\n","LOSS_VAL = []\n","\n","bz = 64\n","lr_init = 1e-3\n","lr_end = 1e-6\n","Epoch = 1  # 1500\n","sigma = 5\n","\n","train_data = torch.from_numpy(train).type(torch.FloatTensor)\n","train_label = torch.from_numpy(train).type(torch.FloatTensor)\n","train_dataset = torch.utils.data.TensorDataset(train_data, train_label)\n","train_dataloader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=bz, shuffle=True)\n","\n","valid_data = torch.from_numpy(eval_spikes_heldin).type(torch.FloatTensor)\n","test_data = torch.from_numpy(test_spikes_heldin).type(torch.FloatTensor)\n","\n","valid_dataset = torch.utils.data.TensorDataset(valid_data, valid_data)\n","valid_dataloader = torch.utils.data.DataLoader(dataset=valid_dataset, batch_size=bz, shuffle=False)\n","test_dataset = torch.utils.data.TensorDataset(test_data, test_data)\n","test_dataloader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=bz, shuffle=False)    \n","train_dataloader_V = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=bz, shuffle=False)"]},{"cell_type":"markdown","metadata":{},"source":["## Training Setting"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["optim_AE = optim.AdamW(model_AE.parameters(), lr=lr_init)\n","optim_HOFW = optim.AdamW(model_HOFW.parameters(), lr=lr_init)\n","\n","scheduler_AE = optim.lr_scheduler.StepLR(optim_AE, step_size=50, gamma=0.95)\n","scheduler_HOFW = optim.lr_scheduler.StepLR(optim_HOFW, step_size=50, gamma=0.95)\n","\n","lossRect = nn.HuberLoss()\n","lossEmbe = nn.HuberLoss()\n","lossHoFw = nn.HuberLoss()\n","\n","model_AE = model_AE.to(device)\n","model_HOFW = model_HOFW.to(device)\n","lossRect = lossRect.to(device)\n","lossEmbe = lossEmbe.to(device)\n","lossHoFw = lossHoFw.to(device)"]},{"cell_type":"markdown","metadata":{},"source":["## Training"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","------Training------\n","epoch[1], Train loss:0.0565, Rloss:0.0564, Eloss:0.0000, HFloss:0.0000\n","\n","------Validation------\n","evaluate:[{'mc_maze_split': {'co-bps': -6.02891846179191, 'vel R2': -0.10263323169558936, 'psth R2': -8442.849241702319, 'fp-bps': -29.748462068350413}}]\n"]}],"source":["print('\\n------Training------')\n","for epoch in range(Epoch):\n","    model_AE.train()\n","    model_HOFW.train()\n","    for n, (Data, Label) in enumerate(train_dataloader):\n","        # process\n","        N_pt = Data.size(0)\n","        spk_data = _shuffle(Data)\n","        spk_data_hi = _GF_pt(spk_data[:, :T, :C], sigma, N_pt, C)\n","        spk_data_hi_1 = spk_data_hi[:, :, :C1]\n","        spk_data_hi_2 = spk_data_hi[:, :, C1:]\n","\n","        # spk_data_LB = _GF_pt(spk_data, sigma, N_pt, C+Co)\n","        spk_data_LB = _GF_pt(spk_data, sigma, N_pt, C+Co, log=False)\n","\n","        spk_data_hi_1 = spk_data_hi_1.to(device)\n","        spk_data_hi_2 = spk_data_hi_2.to(device)\n","        spk_data_LB = spk_data_LB.to(device)\n","        # Rect + HOFW\n","        x1_enc, x2_enc, x1_rec, x2_rec = model_AE(spk_data_hi_1, spk_data_hi_2)\n","        x_rec = torch.cat([x1_rec, x2_rec], axis=-1)\n","        x_rec_TRSP = x_rec.transpose(1, 2)\n","        x_rec_ho, x_rec_fw = model_HOFW(x_rec, x_rec_TRSP)\n","\n","        x_rec_ho_TRSP = x_rec_ho.transpose(1, 2)\n","        x_rec_fw_ho, x_rec_ho_fw = model_HOFW(x_rec_fw, x_rec_ho_TRSP)\n","\n","        FR_fd = torch.cat([x_rec_fw, x_rec_ho_fw], axis=-1)\n","        FR_bd = torch.cat([x_rec, x_rec_ho], axis=-1)\n","        FR = torch.cat([FR_fd, FR_bd], axis=1)            \n","        # update\n","        Rloss = lossRect(FR, spk_data_LB)\n","        Eloss = lossEmbe(x1_enc, x2_enc)\n","        HFloss = lossHoFw(x_rec_fw_ho, x_rec_ho_fw)\n","        loss = Rloss + Eloss + HFloss\n","\n","        optim_AE.zero_grad()\n","        optim_HOFW.zero_grad()\n","        loss.backward()\n","        optim_AE.step()\n","        optim_HOFW.step()\n","\n","    print('epoch[{}], Train loss:{:.4f}, Rloss:{:.4f}, Eloss:{:.4f}, HFloss:{:.4f}'\\\n","        .format(epoch+1, loss.item(), Rloss.item(), Eloss.item(), HFloss.item())) \n","    LOSS_TRA.append(\n","        {'tot': loss.item(), \n","        'rect': Rloss.item(),\n","        'emb': Eloss.item(),\n","        'hf': HFloss.item()}\n","    )     \n","    if loss.item() < 0.01:\n","        print('traning loss convg')\n","        break       \n","    \n","print('\\n------Validation------')\n","model_AE.eval()\n","model_HOFW.eval()\n","with torch.no_grad():\n","    VAL_L, TRA_L = [], []\n","    # ==================================\n","    for n, (Val_Data, Tes_Data) in enumerate(valid_dataloader):\n","        # process\n","        n_val = Val_Data.size(0)\n","        Val_Data_GF = _GF_pt(Val_Data, sigma, n_val, C)\n","        spk_data_hi_1_val = Val_Data_GF[:, :, :C1]\n","        spk_data_hi_2_val = Val_Data_GF[:, :, C1:] \n","\n","        spk_data_hi_1_val = spk_data_hi_1_val.to(device)\n","        spk_data_hi_2_val = spk_data_hi_2_val.to(device)\n","        \n","        _, _, x1_val_rec, x2_val_rec = model_AE(spk_data_hi_1_val, spk_data_hi_2_val)\n","        x_val_rec = torch.cat([x1_val_rec, x2_val_rec], axis=-1)\n","        x_val_rec_TRSP = x_val_rec.transpose(1, 2)\n","        x_val_rec_ho, x_val_rec_fw = model_HOFW(x_val_rec, x_val_rec_TRSP)\n","\n","        x_val_rec_ho_TRSP = x_val_rec_ho.transpose(1, 2)\n","        x_val_rec_fw_ho, x_val_rec_ho_fw = model_HOFW(x_val_rec_fw, x_val_rec_ho_TRSP)\n","\n","        FR_val_fd = torch.cat([x_val_rec_fw, x_val_rec_ho_fw], axis=-1)\n","        FR_val_bd = torch.cat([x_val_rec, x_val_rec_ho], axis=-1)\n","        FR_val = torch.cat([FR_val_fd, FR_val_bd], axis=1)\n","\n","        VAL_L.append(FR_val.cpu().data.numpy())\n","    valid_inferred = np.vstack(VAL_L)\n","    # ==================================\n","    # ==================================\n","    for n, (Data_V, Label_V) in enumerate(train_dataloader_V):\n","        # process\n","        n_tra = Data_V.size(0)\n","        Data_V_hi = Data_V[:, :T, :C]\n","        Data_V_hi_GF = _GF_pt(Data_V_hi, sigma, n_tra, C)  \n","        spk_data_hi_1_tra = Data_V_hi_GF[:, :, :C1]\n","        spk_data_hi_2_tra = Data_V_hi_GF[:, :, C1:] \n","\n","        spk_data_hi_1_tra = spk_data_hi_1_tra.to(device)\n","        spk_data_hi_2_tra = spk_data_hi_2_tra.to(device)\n","        \n","        _, _, x1_tra_rec, x2_tra_rec = model_AE(spk_data_hi_1_tra, spk_data_hi_2_tra)\n","        x_tra_rec = torch.cat([x1_tra_rec, x2_tra_rec], axis=-1)\n","        x_tra_rec_TRSP = x_tra_rec.transpose(1, 2)\n","        x_tra_rec_ho, x_tra_rec_fw = model_HOFW(x_tra_rec, x_tra_rec_TRSP)\n","\n","        x_tra_rec_ho_TRSP = x_tra_rec_ho.transpose(1, 2)\n","        x_tra_rec_fw_ho, x_tra_rec_ho_fw = model_HOFW(x_tra_rec_fw, x_tra_rec_ho_TRSP)\n","\n","        FR_tra_fd = torch.cat([x_tra_rec_fw, x_tra_rec_ho_fw], axis=-1)\n","        FR_tra_bd = torch.cat([x_tra_rec, x_tra_rec_ho], axis=-1)\n","        FR_tra = torch.cat([FR_tra_fd, FR_tra_bd], axis=1)\n","\n","        TRA_L.append(FR_tra.cpu().data.numpy())\n","    train_inferred = np.vstack(TRA_L)\n","    # ==================================        \n","    output_dict = _Dict(dataset_name, train_inferred, valid_inferred, T, C)\n","    EVA = evaluate(target_dict, output_dict)\n"," \n","    print('evaluate:{}'.format(EVA))\n","    LOSS_VAL.append(EVA)   "]},{"cell_type":"markdown","metadata":{},"source":["## Testing"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","------Testing------\n"]}],"source":["print('\\n------Testing------')\n","model_AE.eval()\n","model_HOFW.eval()\n","with torch.no_grad():\n","    TES_L = []\n","    # ==================================\n","    for n, (Tes_Data, Val_Data) in enumerate(test_dataloader):\n","        # process\n","        n_tes = Tes_Data.size(0)\n","        Tes_Data_GF = _GF_pt(Tes_Data, sigma, n_tes, C)        \n","        spk_data_hi_1_tes = Tes_Data_GF[:, :, :C1]\n","        spk_data_hi_2_tes = Tes_Data_GF[:, :, C1:] \n","\n","        spk_data_hi_1_tes = spk_data_hi_1_tes.to(device)\n","        spk_data_hi_2_tes = spk_data_hi_2_tes.to(device)\n","        \n","        _, _, x1_tes_rec, x2_tes_rec = model_AE(spk_data_hi_1_tes, spk_data_hi_2_tes)\n","        x_tes_rec = torch.cat([x1_tes_rec, x2_tes_rec], axis=-1)\n","        x_tes_rec_TRSP = x_tes_rec.transpose(1, 2)\n","        x_tes_rec_ho, x_tes_rec_fw = model_HOFW(x_tes_rec, x_tes_rec_TRSP)\n","\n","        x_tes_rec_ho_TRSP = x_tes_rec_ho.transpose(1, 2)\n","        x_tes_rec_fw_ho, x_tes_rec_ho_fw = model_HOFW(x_tes_rec_fw, x_tes_rec_ho_TRSP)\n","\n","        FR_tes_fd = torch.cat([x_tes_rec_fw, x_tes_rec_ho_fw], axis=-1)\n","        FR_tes_bd = torch.cat([x_tes_rec, x_tes_rec_ho], axis=-1)\n","        FR_tes = torch.cat([FR_tes_fd, FR_tes_bd], axis=1)\n","\n","        TES_L.append(FR_tes.cpu().data.numpy())\n","    test_inferred = np.vstack(TES_L)\n","    # =================================="]},{"cell_type":"markdown","metadata":{},"source":["## Saving"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","------Saving------\n"]},{"name":"stderr","output_type":"stream","text":["[('mc_maze', 'train_rates_heldin'), ('mc_maze', 'train_rates_heldout'), ('mc_maze', 'eval_rates_heldin'), ('mc_maze', 'eval_rates_heldout'), ('mc_maze', 'eval_rates_heldin_forward'), ('mc_maze', 'eval_rates_heldout_forward')] already found in ./inFR/finnal_SML1seq.h5. Overwriting...\n"]},{"name":"stdout","output_type":"stream","text":["train_rates_heldin: [(2295, 140, 137)]\n","train_rates_heldout: [(2295, 140, 45)]\n","eval_rates_heldin: [(574, 140, 137)]\n","eval_rates_heldout: [(574, 140, 45)]\n","eval_rates_heldin_forward: [(574, 40, 137)]\n","eval_rates_heldout_forward: [(574, 40, 45)]\n"]}],"source":["print('\\n------Saving------')\n","output_dict = _Dict(dataset_name, np.vstack([train_inferred, valid_inferred]), test_inferred, T, C)\n","\n","for k in output_dict[dataset_name].keys():\n","    print(k + ': [{}]'.format(output_dict[dataset_name][k].shape))\n","\n","save_to_h5(output_dict, './inFR/finnal_SML1seq.h5', overwrite=True)\n","\n","tra_npy_fn = dataset_name + '_train_loss_SML1seq.npy'\n","val_npy_fn = dataset_name + '_valid_loss_SML1seq.npy'\n","np.save(os.path.join('./inFR', tra_npy_fn), LOSS_TRA)\n","np.save(os.path.join('./inFR', val_npy_fn), LOSS_VAL)"]},{"cell_type":"markdown","metadata":{},"source":["## Time Counting"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","It cost 89.4977 sec\n"]}],"source":["tEnd = time.time()\n","print (\"\\n\" + \"It cost {:.4f} sec\" .format(tEnd-tStart))"]}],"metadata":{"interpreter":{"hash":"ace6faa92082bcf39b13832235d8cfc943222f9017d6df106d112a19597cfb15"},"kernelspec":{"display_name":"Python 3.8.2 64-bit ('3.8.2-venv': venv)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.2"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
